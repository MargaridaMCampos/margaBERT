{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done importing data\n",
      "Tokenizing\n",
      "MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertForMultiLabelSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiLabelSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20: [=========================>......] - ETA 11.3s\n",
      "20/20: [=========================>......] - ETA 11.1s\n",
      "20/20: [=========================>......] - ETA 11.0s\n",
      "SQUAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20: [=========================>......] - ETA 11.5s\n",
      "20/20: [=========================>......] - ETA 11.0s\n",
      "20/20: [=========================>......] - ETA 10.4s\n",
      "BioAsq\n",
      "20/20: [=========================>......] - ETA 6.9ss\n",
      "20/20: [=========================>......] - ETA 6.0ss\n",
      "20/20: [=========================>......] - ETA 6.2ss\n",
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[145]:\n",
    "\n",
    "\n",
    "import torch\n",
    "import random\n",
    "#random.seed(1995)\n",
    "\n",
    "\n",
    "# # Import sample of all datasets:\n",
    "# \n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "#SQuAD\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def read_squad(path):\n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "\n",
    "    return contexts, questions, answers\n",
    "\n",
    "squad_contexts, squad_questions, squad_answers = read_squad('preproc_datasets/train-v2.0.json')\n",
    "random_index = random.sample(range(len(squad_answers)),20)\n",
    "squad_contexts = [squad_contexts[index] for index in random_index]\n",
    "squad_questions = [squad_questions[index] for index in random_index]\n",
    "squad_answers = [squad_answers[index] for index in random_index]\n",
    "\n",
    "\n",
    "# In[64]:\n",
    "\n",
    "\n",
    "# MNLI \n",
    "\n",
    "import jsonlines\n",
    "\n",
    "def parse_mnli(path):\n",
    "    sentences_a = []\n",
    "    sentences_b = []\n",
    "    labels = []\n",
    "    with open(path, \"r+\", encoding=\"utf8\") as f:\n",
    "        for item in jsonlines.Reader(f):\n",
    "            sentences_a.append(item['sentence1'])\n",
    "            sentences_b.append(item['sentence2'])\n",
    "            labels.append(item['gold_label'])\n",
    "    \n",
    "    return sentences_a,sentences_b,labels\n",
    "\n",
    "mnli_a, mnli_b, mnli_labels = parse_mnli('./preproc_datasets/multinli_1.0_train.json')\n",
    "random_index = random.sample(range(len(mnli_a)),20)\n",
    "mnli_a = [mnli_a[index] for index in random_index]\n",
    "mnli_b = [mnli_b[index] for index in random_index]\n",
    "mnli_labels = [mnli_labels[index] for index in random_index]\n",
    "\n",
    "label_encode = {'contradiction': 0,\n",
    "                'neutral': 1,\n",
    "                'entailment': 2}\n",
    "mnli_labels = [label_encode[label] for label in mnli_labels]\n",
    "\n",
    "\n",
    "# In[65]:\n",
    "\n",
    "\n",
    "# BioASQ Factoid\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "with open('preproc_datasets/BioASQ-train-factoid-8b-snippet-annotated.json', 'rb') as f:\n",
    "    bio_factoid_raw = json.load(f)['data'][0]['paragraphs']\n",
    "bio_factoid = [q['qas'][0] for q in bio_factoid_raw]\n",
    "for i in range(len(bio_factoid)):\n",
    "    bio_factoid[i]['context'] = bio_factoid_raw[i]['context']\n",
    "bio_factoid_df = pd.DataFrame.from_dict(bio_factoid)\n",
    "bio_factoid_df['answer_text'] = bio_factoid_df.answers.map(lambda x: x[0]['text'])\n",
    "bio_factoid_df['answer_start'] = bio_factoid_df.answers.map(lambda x: x[0]['answer_start'])\n",
    "bio_factoid_df.drop('answers', axis = 1, inplace = True)\n",
    "#bio_factoid_df.head()\n",
    "\n",
    "bio_factoid_questions = list(bio_factoid_df.question)\n",
    "bio_factoid_contexts = list(bio_factoid_df.context)\n",
    "bio_factoid_answers = [{'text': row['answer_text'],\n",
    "                       'answer_start': row['answer_start']} \n",
    "                      for index, row in bio_factoid_df.iterrows()]\n",
    "# Sampling\n",
    "random_index = random.sample(range(len(bio_factoid_questions)),20)\n",
    "\n",
    "bio_factoid_questions = [bio_factoid_questions[index] for index in random_index]\n",
    "bio_factoid_contexts = [bio_factoid_contexts[index] for index in random_index]\n",
    "bio_factoid_answers = [bio_factoid_answers[index] for index in random_index]\n",
    "\n",
    "\n",
    "\n",
    "print('Done importing data')\n",
    "\n",
    "\n",
    "# Import Model and Tokenizer \n",
    "\n",
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\", \n",
    "                                          do_lower_case=True,\n",
    "                                          padding = True,\n",
    "                                          truncation=True,\n",
    "                                          add_special_tokens = True,\n",
    "                                          model_max_length = 1000000000)\n",
    "\n",
    "tokenizer_fast = BertTokenizerFast.from_pretrained('dmis-lab/biobert-base-cased-v1.1', \n",
    "                                          do_lower_case=True,\n",
    "                                          padding = True,\n",
    "                                          truncation=True,\n",
    "                                          add_special_tokens = True,\n",
    "                                          model_max_length = 1000000000)\n",
    "\n",
    "\n",
    "# In[67]:\n",
    "\n",
    "\n",
    "# SQuAD processing functions\n",
    "from train.squad_processing import add_end_idx, add_token_positions\n",
    "\n",
    "add_end_idx(squad_answers,squad_contexts)\n",
    "add_end_idx(bio_factoid_answers,bio_factoid_contexts)\n",
    "\n",
    "\n",
    "# # Tokenization\n",
    "\n",
    "# In[68]:\n",
    "\n",
    "print(\"Tokenizing\")\n",
    "\n",
    "# SQuAD\n",
    "\n",
    "squad_encodings = tokenizer_fast(squad_contexts,squad_questions,\n",
    "                                 add_special_tokens=True,\n",
    "                                 truncation=True,\n",
    "                                 padding=True,\n",
    "                                 max_length=500)\n",
    "\n",
    "# Processing of token positions\n",
    "add_token_positions(squad_encodings, squad_answers, tokenizer_fast)\n",
    "\n",
    "\n",
    "# In[69]:\n",
    "\n",
    "\n",
    "# MNLI\n",
    "\n",
    "mnli_encodings = tokenizer(mnli_a,mnli_b, \n",
    "                        add_special_tokens=True,\n",
    "                        max_length=500,\n",
    "                        truncation=True, \n",
    "                        padding=True)\n",
    "mnli_encodings['labels'] = mnli_labels\n",
    "\n",
    "\n",
    "# In[70]:\n",
    "\n",
    "\n",
    "# BioASQ\n",
    "\n",
    "bio_factoid_encodings = tokenizer_fast(bio_factoid_contexts,bio_factoid_questions,\n",
    "                               add_special_tokens=True,\n",
    "                               truncation=True,\n",
    "                               padding=True,\n",
    "                               max_length=500)\n",
    "\n",
    "# Processing of token positions\n",
    "add_token_positions(bio_factoid_encodings, bio_factoid_answers,tokenizer_fast)\n",
    "\n",
    "\n",
    "# In[71]:\n",
    "\n",
    "\n",
    "# Defining Datasets \n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MnliDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print(self.encodings['start_positions'][idx])\n",
    "        #{key: torch.tensor(val[idx], dtype = torch.long) for key, val in self.encodings.items()}\n",
    "        return {'input_ids': torch.tensor(self.encodings['input_ids'][idx], dtype = torch.long),\n",
    "                'attention_mask': torch.tensor(self.encodings['attention_mask'][idx], dtype = torch.long),\n",
    "                'token_type_ids': torch.tensor(self.encodings['token_type_ids'][idx], dtype = torch.long),\n",
    "                'labels': torch.tensor(self.encodings['labels'][idx], dtype = torch.long)\n",
    "               }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "class SquadDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print(self.encodings['start_positions'][idx])\n",
    "         #{key: torch.tensor(val[idx], dtype = torch.long) for key, val in self.encodings.items()}\n",
    "        return {'input_ids':torch.tensor(self.encodings['input_ids'][idx],dtype = torch.long),\n",
    "         'attention_mask':torch.tensor(self.encodings['attention_mask'][idx],dtype = torch.long),\n",
    "         'start_positions':torch.tensor(self.encodings['start_positions'][idx],dtype = torch.long),\n",
    "         'end_positions':torch.tensor(self.encodings['end_positions'][idx],dtype = torch.long)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "\n",
    "# In[72]:\n",
    "\n",
    "print('MNLI')\n",
    "\n",
    "train_mnli = MnliDataset(mnli_encodings)\n",
    "train_squad = SquadDataset(squad_encodings)\n",
    "train_bio_factoid = SquadDataset(bio_factoid_encodings)\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "# In[106]:\n",
    "\n",
    "\n",
    "class BertForMultiLabelSequenceClassification(BertPreTrainedModel):\n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_labels=3):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, num_labels)\n",
    "        #self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        pooled_output = self.bert(input_ids, token_type_ids, attention_mask)[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "    \n",
    "        return logits\n",
    "        \n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "# In[101]:\n",
    "\n",
    "\n",
    "mnli_model = BertForMultiLabelSequenceClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n",
    "\n",
    "# In[98]:\n",
    "\n",
    "from torch.nn import DataParallel\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "mnli_model.to(device)\n",
    "mnli_model.train()\n",
    "\n",
    "#mnli_model = DataParallel(mnli_model, device_ids = [0,1,2,3])\n",
    "\n",
    "train_loader_mnli = DataLoader(train_mnli, batch_size= 4, shuffle=True)\n",
    "\n",
    "optim = AdamW(mnli_model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "# In[105]:\n",
    "\n",
    "\n",
    "from barbar import Bar\n",
    "for epoch in range(3):\n",
    "    for i,batch in enumerate(Bar(train_loader_mnli)):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "        attention_mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n",
    "        labels = batch['labels'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = mnli_model(input_ids, \n",
    "                                attention_mask=attention_mask, \n",
    "                                token_type_ids = token_type_ids,\n",
    "                                labels = labels)\n",
    "\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(outputs, labels)\n",
    "        #loss = outputs.loss\n",
    "        loss.sum().backward()\n",
    "        optim.step()\n",
    "mnli_model.eval()\n",
    "\n",
    "\n",
    "# In[111]:\n",
    "\n",
    "\n",
    "from transformers.modeling_outputs import QuestionAnsweringModelOutput\n",
    "\n",
    "\n",
    "# In[112]:\n",
    "\n",
    "\n",
    "class BertForQuestionAnswering(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        start_positions=None,\n",
    "        end_positions=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
    "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
    "            sequence are not taken into account for computing the loss.\n",
    "        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
    "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
    "            sequence are not taken into account for computing the loss.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions.clamp_(0, ignored_index)\n",
    "            end_positions.clamp_(0, ignored_index)\n",
    "\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (start_logits, end_logits) + outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output\n",
    "\n",
    "        return QuestionAnsweringModelOutput(\n",
    "            loss=total_loss,\n",
    "            start_logits=start_logits,\n",
    "            end_logits=end_logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "# In[116]:\n",
    "\n",
    "print('SQUAD')\n",
    "squad_model = BertForQuestionAnswering.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n",
    "\n",
    "# In[120]:\n",
    "\n",
    "\n",
    "squad_model.load_state_dict(mnli_model.state_dict(), \n",
    "                            strict=False)\n",
    "\n",
    "\n",
    "# In[121]:\n",
    "\n",
    "\n",
    "squad_model.to(device)\n",
    "squad_model.train()\n",
    "\n",
    "#squad_model = DataParallel(squad_model, device_ids = [0,1,2,3])\n",
    "\n",
    "\n",
    "train_loader_squad = DataLoader(train_squad, batch_size=4, shuffle=True)\n",
    "\n",
    "optim = AdamW(squad_model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "# In[122]:\n",
    "\n",
    "\n",
    "for epoch in range(3):\n",
    "    for i,batch in enumerate(Bar(train_loader_squad)):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "        attention_mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "        start_positions = batch['start_positions'].to(device, dtype = torch.long)\n",
    "        end_positions = batch['end_positions'].to(device, dtype = torch.long)\n",
    "        outputs = squad_model(input_ids, \n",
    "                        attention_mask=attention_mask, \n",
    "                        start_positions=start_positions, \n",
    "                        end_positions=end_positions)\n",
    "        loss = outputs[0]\n",
    "        loss.sum().backward()\n",
    "        optim.step()\n",
    "#squad_model.eval()\n",
    "\n",
    "\n",
    "# In[123]:\n",
    "print('BioAsq')\n",
    "\n",
    "#squad_model.to(device)\n",
    "squad_model.train()\n",
    "\n",
    "train_loader_factoid = DataLoader(train_bio_factoid, batch_size=4, shuffle=True)\n",
    "\n",
    "#optim = AdamW(squad_model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "# In[124]:\n",
    "\n",
    "\n",
    "# Train on BioAsq\n",
    "\n",
    "for epoch in range(3):\n",
    "    for i,batch in enumerate(Bar(train_loader_factoid)):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "        attention_mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "        start_positions = batch['start_positions'].to(device, dtype = torch.long)\n",
    "        end_positions = batch['end_positions'].to(device, dtype = torch.long)\n",
    "        outputs = squad_model(input_ids, \n",
    "                        attention_mask=attention_mask, \n",
    "                        start_positions=start_positions, \n",
    "                        end_positions=end_positions)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "squad_model.eval()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "print('Saving...')\n",
    "\n",
    "torch.save({\n",
    "            'epoch': 3,\n",
    "            'model_state_dict': squad_model.state_dict(),\n",
    "            'optimizer_state_dict': optim.state_dict(),\n",
    "            'loss': loss,\n",
    "            },'try_small_mnli_squad.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question = \"What does the pembrolizumab companion diagnostic test assess?\"\n",
    "test_context = \"The initial approval and subsequent studies of pembrolizumab required and utilized a companion diagnostic test, Dako's IHC 22C3, to assess PD-L1 status of patients.\"\n",
    "\n",
    "    \n",
    "\n",
    "inputs = tokenizer_fast(test_question, \n",
    "                   test_context, \n",
    "                   truncation=True, \n",
    "                   padding=True,\n",
    "                   max_length=500, \n",
    "                   return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "text_tokens = tokenizer_fast.convert_ids_to_tokens(input_ids)\n",
    "outputs = squad_model(**inputs)\n",
    "answer_start_scores = outputs.start_logits\n",
    "# Get the most likely beginning of answer \n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "answer_end_scores = outputs.end_logits[0][answer_start:]\n",
    "# Get the most likely end of answer \n",
    "answer_end = torch.argmax(answer_end_scores) + 1  \n",
    "answer = tokenizer_fast.convert_tokens_to_string(\n",
    "    tokenizer_fast.convert_ids_to_tokens(\n",
    "        input_ids[answer_start:(answer_start+answer_end+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=tensor(1.0103, grad_fn=<DivBackward0>), start_logits=tensor([[-3.4516,  2.7427, -0.4152,  0.1761,  0.1918, -0.4189, -2.5025, -3.1370,\n",
       "         -1.4827, -2.6535, -0.9092, -2.6737, -2.3905, -0.0488, -2.1532, -0.2783,\n",
       "          0.5209, -1.6242, -0.5960, -1.5855,  0.2661, -1.0698,  0.9707, -0.4045,\n",
       "          1.7623,  0.5302, -1.9339, -0.8773, -2.2173, -2.8530, -2.7489,  2.0464,\n",
       "         -0.1324, -1.4405, -2.3707, -3.6621, -4.1442, -1.4680, -3.8297, -2.3066,\n",
       "         -3.2082, -3.5331, -2.3637, -3.3932, -3.1683,  1.2367,  0.0141, -2.1442,\n",
       "         -2.4285, -4.0495, -1.9655, -3.9713, -4.2164, -4.1126, -3.9640, -4.2063,\n",
       "         -4.2654, -4.1014, -4.1493, -4.1455, -4.1077, -4.1424, -4.0190, -4.3712,\n",
       "         -4.0388, -4.2502, -4.2070, -3.8898, -4.2001, -4.1015, -4.3487, -4.2094,\n",
       "         -4.2735, -4.0721, -4.2352, -4.2720, -4.2234, -4.1372, -4.1905, -4.2122,\n",
       "         -4.2407, -4.1701, -4.2063, -4.0683, -4.0178, -4.2388, -4.1603, -4.0355,\n",
       "         -4.1857, -4.1195, -4.0310, -4.3183, -4.1517, -4.4163, -4.3920, -4.2559,\n",
       "         -4.2167, -4.2854, -4.1407, -4.2710, -4.2137, -4.3690, -4.2302, -4.0532,\n",
       "         -4.2050, -4.1405, -4.2055, -4.0336, -4.0576, -4.1809, -4.1740, -4.0916,\n",
       "         -4.4839, -4.1441, -4.1825, -4.1450, -4.2333, -4.2702, -4.2070, -4.1308,\n",
       "         -4.0032, -4.0243, -4.1057, -4.2028, -4.1345, -3.9914, -4.3136, -3.9874,\n",
       "         -4.4727, -4.2811, -4.1452, -4.2755, -4.2265, -4.1487, -4.2800, -4.0758,\n",
       "         -3.8560, -4.2789, -4.2768, -3.9975, -3.9622, -4.1492, -4.2351, -4.0509,\n",
       "         -4.1497, -4.0150, -4.3864, -4.1531, -4.1035, -4.1765, -4.2351, -4.0932,\n",
       "         -4.1455, -4.3379, -3.9799, -4.0870, -4.1399, -4.1932, -4.3571, -4.3992,\n",
       "         -4.1235, -4.3076, -4.1813, -4.3100, -4.2198, -4.2780, -4.1585, -4.1007,\n",
       "         -4.1904, -4.1043, -4.0224, -4.0677, -3.9966, -4.1372, -4.3061, -4.3997,\n",
       "         -4.1845, -4.2669, -4.1593, -4.1960, -4.0497, -4.0473, -3.9723, -4.3297,\n",
       "         -4.1089, -4.1424, -4.3413, -4.1838, -4.1688, -4.2561, -4.1694, -4.1863,\n",
       "         -4.2801, -4.1662, -4.2443, -4.3124, -3.9522, -4.2943, -3.8030, -4.3816,\n",
       "         -4.3074, -4.3774, -4.1975, -4.2524, -4.0627, -4.2348, -4.2729, -4.3154,\n",
       "         -4.3129, -4.1516, -4.3425, -4.2259, -3.9737],\n",
       "        [-3.6397, -0.7874, -0.2582, -1.3288, -3.2103, -3.6475, -2.9679, -2.3000,\n",
       "         -0.4291,  2.0427,  4.0268,  0.4664, -1.1401, -3.0358, -2.2046, -0.6299,\n",
       "         -1.3059, -2.0719, -2.0988, -1.3364, -2.8732, -2.3729, -2.0941, -0.2385,\n",
       "         -0.3335, -2.3435, -1.0741, -2.5520, -0.8505, -0.4162, -1.3466, -1.0643,\n",
       "         -3.3315, -2.2681, -0.6737, -1.9735, -1.7305, -3.3356, -2.1961, -3.4836,\n",
       "         -1.1566, -0.6822, -2.0070, -1.5966, -3.1755, -2.5219, -2.8145, -2.9571,\n",
       "         -3.2861, -0.4037, -1.0685, -1.7330, -3.4983, -3.7241, -2.0672, -3.8748,\n",
       "         -4.2217, -3.8859, -4.1209, -3.8962, -3.9514, -3.9018, -3.6958, -4.0041,\n",
       "         -3.8437, -4.0769, -3.8309, -4.1819, -3.9333, -3.6719, -3.6372, -3.7995,\n",
       "         -3.8212, -4.0034, -4.0301, -3.4975, -3.9578, -3.6914, -3.9099, -3.7131,\n",
       "         -3.9269, -3.5923, -3.8895, -4.0919, -3.6615, -3.8801, -3.8154, -3.9192,\n",
       "         -4.0045, -4.0105, -4.1529, -3.7845, -3.8909, -3.9022, -3.8390, -3.9150,\n",
       "         -4.0264, -3.8190, -3.6778, -3.9181, -3.7592, -3.9885, -3.9792, -3.8622,\n",
       "         -3.8759, -4.0145, -3.8840, -3.8845, -3.8055, -3.8814, -4.0956, -3.9351,\n",
       "         -3.8342, -4.0535, -3.9252, -3.7535, -3.9180, -3.8786, -3.8537, -4.0526,\n",
       "         -3.9284, -4.1124, -4.0114, -3.8243, -3.8527, -3.6595, -3.9640, -4.0720,\n",
       "         -3.9050, -3.8344, -3.9201, -4.0394, -4.1514, -4.1295, -3.6440, -3.9627,\n",
       "         -3.7561, -3.9146, -4.1602, -3.8388, -4.0410, -3.7299, -3.9669, -3.7774,\n",
       "         -3.8008, -3.5979, -3.9560, -3.8272, -3.8640, -3.7318, -4.0053, -3.9586,\n",
       "         -4.0150, -3.9253, -3.9571, -4.0778, -3.9390, -3.9031, -3.8755, -3.9376,\n",
       "         -4.0082, -3.9374, -3.9030, -3.7432, -4.0245, -3.5106, -3.8905, -3.7877,\n",
       "         -3.7907, -3.9885, -4.0148, -4.2006, -4.0593, -4.1166, -3.9356, -3.9966,\n",
       "         -3.8746, -3.9809, -4.0350, -3.9438, -3.9235, -3.5384, -3.9032, -4.0847,\n",
       "         -4.1065, -3.6088, -4.0737, -3.7054, -4.1345, -3.9938, -3.9166, -3.7703,\n",
       "         -3.5479, -3.9355, -3.7818, -3.9200, -3.8280, -3.8805, -4.0880, -3.9627,\n",
       "         -3.9498, -3.8612, -3.9507, -4.0083, -3.9176, -3.7605, -3.8502, -3.9713,\n",
       "         -3.9685, -3.8151, -4.1454, -4.0691, -3.7974],\n",
       "        [-3.7768, -2.0863, -1.9574, -2.8379, -0.8310, -1.7130, -2.3659,  0.3271,\n",
       "         -1.2146,  1.1025,  0.1001,  2.4812,  0.8553, -0.7728,  1.6183, -0.0600,\n",
       "         -0.2405, -1.9857,  2.1196, -0.3399,  1.8698,  0.1776, -3.4366, -2.0110,\n",
       "          0.7452, -0.7696,  0.2100, -0.2086, -2.6692, -1.2341, -1.8626, -1.1384,\n",
       "         -2.7883, -1.6486,  0.4329, -0.4390, -0.5321, -1.3214, -0.2422, -2.7693,\n",
       "         -1.9845, -3.9627, -2.1225, -4.0293, -3.6828, -3.5724, -2.6357, -2.4479,\n",
       "         -2.4099,  1.1589, -1.2531,  0.9660, -0.7995, -1.9234, -2.4606, -0.9175,\n",
       "         -1.4436, -2.0713, -2.7413, -4.0008, -2.0412, -4.2551, -4.0468, -4.0517,\n",
       "         -4.1713, -4.2266, -4.2493, -4.2098, -4.1478, -4.2059, -4.1214, -4.2029,\n",
       "         -4.2055, -4.2258, -4.2124, -4.0178, -3.9245, -3.8957, -4.1166, -4.0518,\n",
       "         -4.1937, -3.9635, -4.0779, -4.1747, -4.0815, -4.2860, -4.0298, -4.1892,\n",
       "         -4.1348, -4.1952, -4.0785, -4.0943, -4.3253, -4.2063, -4.1804, -4.1480,\n",
       "         -4.2009, -4.2425, -4.2133, -3.9495, -4.1787, -4.1514, -4.2237, -4.1436,\n",
       "         -4.1316, -4.1569, -4.3285, -3.8443, -4.1182, -4.0629, -4.2044, -4.1951,\n",
       "         -4.1871, -4.1478, -4.1402, -4.0342, -4.3275, -4.1660, -4.1857, -4.0317,\n",
       "         -3.9818, -4.1056, -4.2446, -4.0733, -4.1411, -3.9940, -4.2981, -3.9458,\n",
       "         -4.2207, -4.1129, -4.0892, -4.1995, -4.2119, -3.8565, -4.1737, -4.1796,\n",
       "         -4.1607, -4.0621, -4.1053, -4.1288, -4.0404, -3.9752, -4.2083, -4.1438,\n",
       "         -4.2224, -4.0541, -4.2857, -4.1169, -4.1419, -4.1667, -4.2043, -4.1136,\n",
       "         -4.2816, -4.2387, -4.1836, -4.0439, -4.2679, -4.1782, -4.2505, -3.9452,\n",
       "         -4.0798, -4.1610, -4.2560, -4.3631, -4.0869, -4.0195, -4.0649, -4.0742,\n",
       "         -4.3128, -3.9433, -4.1014, -3.8981, -4.1407, -4.2509, -4.2426, -4.3314,\n",
       "         -4.1643, -4.1381, -4.1976, -4.1112, -3.9664, -4.0631, -4.1423, -4.0364,\n",
       "         -4.1369, -4.1445, -4.1512, -4.2510, -4.0524, -4.0568, -4.0599, -4.1880,\n",
       "         -3.9800, -4.2038, -4.2102, -4.1506, -4.1816, -4.1650, -4.3036, -4.1648,\n",
       "         -4.1561, -4.1449, -4.1561, -4.1737, -4.1839, -4.1009, -4.1841, -4.0802,\n",
       "         -4.2076, -4.2430, -4.0386, -4.1405, -4.3889],\n",
       "        [-3.7782, -2.1508, -1.7411, -2.4442, -2.6407, -3.2595, -2.4583, -2.5307,\n",
       "         -2.6400, -1.4358, -1.8616, -2.4885, -1.9195, -3.4078, -2.3777, -1.5134,\n",
       "         -2.5930,  0.3793, -1.8517, -1.3675, -2.0931, -2.4815,  0.9625, -1.7317,\n",
       "          0.9379, -0.5771,  0.9768, -1.0227, -0.6939, -0.4096, -2.1346,  3.2961,\n",
       "          1.2941, -0.2222, -2.8375, -3.1447, -3.3067, -3.0310,  1.8402,  0.9647,\n",
       "          0.5664, -0.0862, -0.5202, -0.5167,  0.2501, -0.9618, -3.0550,  3.0809,\n",
       "          1.1393,  1.0916, -0.5885, -2.9875,  0.0135, -1.0918,  0.6296, -0.3195,\n",
       "         -3.9412, -3.9286, -2.4563, -3.8050, -3.3108, -3.6180, -2.8308, -3.4102,\n",
       "         -1.0583, -2.8214, -3.8413, -2.1746, -4.0928, -4.0687, -3.9251, -3.9165,\n",
       "         -4.0576, -3.7506, -3.9226, -4.1899, -4.1291, -4.1001, -4.1083, -3.9601,\n",
       "         -3.9637, -4.1318, -4.2164, -3.9858, -4.0597, -4.1903, -4.3470, -4.1752,\n",
       "         -4.1842, -4.0306, -4.1038, -4.2305, -3.9434, -4.0347, -4.2062, -4.0661,\n",
       "         -4.2062, -4.0829, -4.0119, -3.8303, -3.9484, -4.0851, -4.0484, -3.9161,\n",
       "         -3.8840, -4.0573, -4.2102, -4.0472, -4.2156, -4.1463, -4.4454, -4.0556,\n",
       "         -4.1041, -3.9981, -4.3689, -4.0476, -3.8581, -4.1115, -4.0930, -4.1824,\n",
       "         -4.0749, -4.1137, -3.9994, -3.7505, -4.0997, -3.9375, -4.1105, -4.3739,\n",
       "         -4.0631, -4.2622, -4.1170, -4.0631, -3.9368, -3.8884, -4.1503, -4.0411,\n",
       "         -4.1492, -4.0345, -3.8946, -4.0804, -3.9574, -4.1938, -4.3394, -4.0334,\n",
       "         -3.9297, -3.8116, -4.1646, -4.1014, -4.1977, -4.0957, -4.0452, -4.0137,\n",
       "         -4.0390, -4.0381, -4.0083, -4.0571, -4.2596, -4.1196, -4.0303, -4.0568,\n",
       "         -4.1297, -4.1351, -4.2397, -4.2203, -3.9990, -4.0453, -4.3132, -3.9158,\n",
       "         -4.0423, -3.8482, -4.2216, -3.9218, -3.9366, -4.1176, -4.0555, -4.1648,\n",
       "         -3.8993, -4.1409, -3.8900, -4.0112, -4.2053, -4.1930, -4.0468, -4.1536,\n",
       "         -4.2817, -3.7212, -4.1759, -3.9890, -4.0658, -3.9143, -4.2944, -3.9250,\n",
       "         -4.1340, -3.9433, -4.1115, -3.9834, -4.0050, -4.0407, -3.9832, -4.2305,\n",
       "         -4.1417, -4.3074, -4.0569, -4.0229, -4.1155, -4.1144, -3.9479, -4.1093,\n",
       "         -4.1952, -4.2161, -4.0370, -4.1000, -4.1688]],\n",
       "       grad_fn=<SqueezeBackward1>), end_logits=tensor([[-4.0495, -2.1667, -2.3502, -0.5059, -1.8790,  3.2527, -2.5790, -3.8480,\n",
       "         -3.6792, -4.2305, -2.2471, -2.8162, -3.2424, -1.9228, -2.7156,  0.6548,\n",
       "         -1.7784,  0.6881, -0.5611, -2.3095, -1.5214, -2.3096, -2.2576, -0.3192,\n",
       "         -2.1325, -2.0065,  1.1226, -2.6578, -0.2015, -3.2893, -4.1208, -3.0348,\n",
       "         -1.8845,  0.4612, -2.3202, -3.3026, -4.1331, -2.9817, -4.6381, -2.9875,\n",
       "         -2.9035, -4.0060, -3.5747, -3.7002, -4.3475, -3.7604, -2.6088, -0.5804,\n",
       "         -2.6302, -4.2281, -3.2597, -4.6156, -4.7185, -4.4719, -4.7145, -4.4726,\n",
       "         -4.5094, -4.6780, -4.7333, -4.7004, -4.6989, -4.8156, -4.5492, -4.7544,\n",
       "         -4.9119, -4.7420, -4.5635, -4.6158, -4.5122, -4.7383, -4.7554, -4.8723,\n",
       "         -4.4857, -4.6989, -4.7913, -4.6689, -4.9620, -4.5996, -4.8015, -4.6146,\n",
       "         -4.5952, -4.8175, -4.4818, -4.6061, -4.7582, -4.7681, -4.7082, -4.6888,\n",
       "         -4.5679, -4.7147, -4.7439, -4.8024, -4.7205, -4.4279, -4.5854, -4.6938,\n",
       "         -4.6744, -4.5931, -4.8329, -4.9355, -4.6912, -4.5325, -4.6292, -4.6333,\n",
       "         -4.7572, -4.3027, -4.8262, -4.7601, -4.7907, -4.7294, -4.7269, -4.5173,\n",
       "         -4.7516, -4.8879, -4.5663, -4.5888, -4.5010, -4.7639, -4.6974, -4.7852,\n",
       "         -4.3951, -4.7164, -4.7940, -4.5923, -4.6283, -4.7724, -4.3690, -4.6150,\n",
       "         -4.3511, -4.7427, -4.6154, -4.7602, -4.6815, -4.6758, -4.5250, -4.5794,\n",
       "         -4.6247, -4.7263, -4.6591, -4.6144, -4.5829, -4.4736, -4.8641, -4.5894,\n",
       "         -4.7519, -4.6138, -4.7271, -4.7817, -4.9459, -4.7591, -4.6489, -4.8973,\n",
       "         -4.7185, -4.6502, -4.6460, -4.4774, -4.7169, -4.6990, -4.5498, -4.8254,\n",
       "         -4.8636, -4.5895, -4.8416, -4.4300, -4.5741, -4.7110, -4.4639, -4.2895,\n",
       "         -4.4159, -4.5256, -4.8192, -4.5287, -4.8873, -4.6957, -4.6515, -4.8023,\n",
       "         -4.7496, -4.7116, -4.3472, -4.6230, -4.6163, -4.9157, -5.0002, -4.6192,\n",
       "         -4.5850, -4.7994, -4.6164, -4.6855, -4.5313, -4.5930, -4.6607, -4.8226,\n",
       "         -4.6285, -4.7222, -4.5866, -4.7417, -4.7531, -4.5246, -4.6731, -4.5485,\n",
       "         -4.6199, -4.8147, -4.5864, -4.5799, -4.7738, -4.6259, -4.6091, -4.5715,\n",
       "         -4.5761, -4.6731, -4.4858, -4.6474, -4.6917],\n",
       "        [-4.5661, -4.7306, -3.6216, -2.9413, -2.7596, -4.3761, -4.2766, -3.4351,\n",
       "         -1.9438, -1.6741, -0.6637,  4.4543, -1.2641, -3.5235, -3.2195, -1.9827,\n",
       "         -2.4432, -1.6414, -1.3609, -1.3275, -4.5890, -3.7194, -3.8362, -2.2125,\n",
       "         -2.4265, -3.3061, -0.3092, -3.4217, -2.5413, -2.1160, -0.9651, -0.7272,\n",
       "         -4.2256, -3.4901, -1.5887, -2.3397, -0.8666, -3.4387, -4.0463, -4.7528,\n",
       "         -3.1079, -3.1412, -2.7957, -2.3536, -4.7264, -4.3623, -4.1023, -4.0697,\n",
       "         -4.2189, -4.6308, -3.5973, -3.4623, -3.8125, -4.3630, -3.6932, -4.6993,\n",
       "         -4.6771, -4.9143, -4.6123, -4.6370, -4.8173, -4.8689, -4.9969, -4.7809,\n",
       "         -4.9505, -4.8584, -4.6480, -4.7116, -4.8759, -4.8918, -4.8459, -4.9503,\n",
       "         -4.5590, -4.8311, -4.7104, -4.7784, -4.8826, -4.5295, -4.7208, -4.8447,\n",
       "         -4.9856, -4.8295, -4.8045, -4.6710, -4.6720, -4.8454, -5.0182, -4.5505,\n",
       "         -4.8463, -4.8933, -4.8366, -4.8233, -4.6947, -4.8538, -4.8104, -4.6627,\n",
       "         -4.8199, -4.7181, -4.8156, -4.7772, -4.6824, -4.9501, -4.7419, -4.8376,\n",
       "         -4.7529, -4.8876, -4.8946, -4.7805, -4.7458, -4.6593, -4.6459, -4.6674,\n",
       "         -4.8516, -4.7080, -4.8989, -5.0679, -4.8826, -4.8459, -4.7751, -4.8235,\n",
       "         -4.8867, -4.4829, -4.7262, -4.8680, -4.7854, -4.7050, -4.6862, -4.5440,\n",
       "         -4.8089, -4.9152, -4.5762, -4.6407, -4.6473, -4.7981, -4.8734, -4.8511,\n",
       "         -4.7773, -5.1505, -4.5468, -4.9042, -4.7533, -4.8566, -4.7238, -4.7830,\n",
       "         -4.8025, -4.8904, -4.7233, -4.7031, -5.0100, -4.7882, -4.4488, -4.8609,\n",
       "         -4.6158, -4.8472, -4.9527, -4.7600, -4.7305, -4.8260, -4.9508, -4.9160,\n",
       "         -4.9057, -4.9173, -4.6781, -4.8235, -4.8658, -4.7816, -4.8293, -4.8178,\n",
       "         -4.7080, -5.0219, -4.6478, -4.5200, -4.6457, -4.7996, -4.6902, -4.7838,\n",
       "         -4.5205, -4.9718, -4.7975, -4.7842, -4.9216, -4.8187, -4.7290, -4.8431,\n",
       "         -4.7789, -4.9712, -4.7586, -5.0864, -4.8442, -4.6976, -4.9072, -4.9350,\n",
       "         -4.8802, -4.6627, -4.8484, -4.8775, -4.9240, -4.7331, -4.6591, -4.9586,\n",
       "         -4.9198, -4.7603, -4.8655, -4.9522, -4.8708, -4.9622, -4.8166, -4.7630,\n",
       "         -4.7088, -4.6660, -4.6829, -4.9424, -4.9957],\n",
       "        [-4.2269, -2.7221, -3.0644, -3.7924, -1.9406, -1.6363, -1.4548, -0.9245,\n",
       "         -1.4465, -0.6145,  1.4033,  0.6791,  2.3252, -0.7145, -1.4141,  0.4392,\n",
       "          1.7880, -2.4373, -0.7827, -1.1401, -0.6627,  3.3086, -3.8519, -2.8356,\n",
       "         -0.8095, -1.3586, -0.3702,  0.0890, -3.0659, -1.6989, -3.1390, -1.6786,\n",
       "         -3.3476, -2.7777, -1.4836, -1.3423, -1.1189, -2.4543,  0.3544, -3.3814,\n",
       "         -2.4904, -4.3656, -2.9183, -4.4324, -4.1946, -4.1917, -2.9418, -2.7788,\n",
       "         -3.5385, -2.1443, -2.6429, -0.9681,  0.9542, -1.1485, -2.5935, -2.4834,\n",
       "         -2.8119, -0.5892, -1.6151, -4.3917, -2.8598, -4.7114, -4.7909, -4.4630,\n",
       "         -4.5027, -4.5987, -4.4072, -4.4232, -4.4963, -4.2815, -4.6891, -4.4548,\n",
       "         -4.4289, -4.4366, -4.6498, -4.7499, -4.7941, -4.8102, -4.6855, -4.7198,\n",
       "         -4.7490, -4.3975, -4.5716, -4.6459, -4.7408, -4.6972, -4.5809, -4.6919,\n",
       "         -4.7216, -4.4494, -4.4177, -4.7086, -4.6861, -4.6331, -4.6969, -4.4966,\n",
       "         -4.6537, -4.5242, -4.7881, -4.5764, -4.8430, -4.4355, -4.5293, -4.8533,\n",
       "         -4.2959, -4.5912, -4.5289, -4.4145, -4.5397, -4.7438, -4.1918, -4.4469,\n",
       "         -4.5811, -4.5831, -4.5515, -4.6136, -4.4408, -4.7751, -4.8759, -4.5454,\n",
       "         -4.7980, -4.7495, -4.3664, -4.6426, -4.6971, -4.5281, -4.5583, -4.7087,\n",
       "         -4.5015, -4.5352, -4.3514, -4.5282, -4.6387, -4.8953, -4.4160, -4.5939,\n",
       "         -4.4161, -4.3753, -4.5250, -4.4976, -4.4632, -4.4641, -4.6611, -4.7350,\n",
       "         -4.6387, -4.6352, -4.5828, -4.8376, -4.7389, -4.5347, -4.5387, -4.6615,\n",
       "         -4.5756, -4.8564, -4.6797, -4.4823, -4.6565, -4.7603, -4.7487, -4.8028,\n",
       "         -4.6776, -4.3019, -4.5878, -4.2989, -4.6953, -4.6449, -4.5032, -4.2309,\n",
       "         -4.5500, -4.7508, -4.7895, -4.7152, -4.7830, -4.8046, -4.4497, -4.5867,\n",
       "         -4.4066, -4.7547, -4.6458, -4.7129, -4.5363, -4.6046, -4.5871, -4.4194,\n",
       "         -4.6120, -4.3800, -4.6759, -4.4579, -4.5397, -4.8094, -4.4040, -4.8611,\n",
       "         -4.8586, -4.5827, -4.4288, -4.7077, -4.4723, -4.5857, -4.6614, -4.7270,\n",
       "         -4.4437, -4.5123, -4.5382, -4.3972, -4.6725, -4.6598, -4.5203, -4.6528,\n",
       "         -4.5667, -4.5617, -4.4588, -4.7439, -4.5686],\n",
       "        [-4.4835, -4.2856, -1.4843, -3.0967, -3.1085, -3.9910, -3.5742, -3.6463,\n",
       "         -3.3674, -2.7278, -3.1420, -2.8217, -1.5702, -3.7135, -3.7262, -2.3994,\n",
       "         -4.0723, -2.7847, -2.5715, -1.1399, -3.4028, -3.8732, -1.3445, -3.6681,\n",
       "         -2.3079, -1.7463, -1.3575, -1.2819, -1.4290,  2.2079, -2.9062, -1.5490,\n",
       "         -1.1695,  2.9072, -2.2259, -3.2790, -4.2463, -4.2035, -2.4499, -0.8721,\n",
       "         -1.2742, -1.5826,  0.0286, -0.6005,  1.8433,  2.1915, -3.9165, -1.5655,\n",
       "          0.0688,  1.5919,  3.8304, -3.7994, -3.3947, -0.9239, -0.9525, -0.0823,\n",
       "         -2.6731, -4.0536, -3.1877, -4.4500, -4.0232, -4.4109, -4.0031, -4.0286,\n",
       "         -3.5944, -2.7928, -4.3101, -3.2534, -4.6262, -4.7199, -4.6346, -4.4588,\n",
       "         -4.7034, -4.7116, -4.6186, -4.4106, -4.6381, -4.5243, -4.6757, -4.7466,\n",
       "         -4.4076, -4.1818, -4.4968, -4.6207, -4.4912, -4.5518, -4.5090, -4.6296,\n",
       "         -4.5504, -4.4553, -4.3585, -4.4126, -4.4856, -4.7043, -4.4874, -4.5406,\n",
       "         -4.5031, -4.6495, -4.7190, -4.8266, -4.7098, -4.4833, -4.4876, -4.3194,\n",
       "         -4.5205, -4.2536, -4.5210, -4.6439, -4.5302, -4.6642, -4.2764, -4.6491,\n",
       "         -4.6607, -4.4538, -4.4782, -4.1867, -4.6783, -4.0401, -4.5641, -4.3864,\n",
       "         -4.5308, -4.6483, -4.4472, -4.5707, -4.5692, -4.4440, -4.6066, -4.4739,\n",
       "         -4.6173, -4.6557, -4.4776, -4.4554, -4.4617, -4.4982, -4.5461, -4.4607,\n",
       "         -4.3597, -4.5081, -4.5272, -4.5641, -4.6383, -4.5875, -4.4341, -4.5415,\n",
       "         -4.7618, -4.7668, -4.7220, -4.4227, -4.6311, -4.4317, -4.6206, -4.4609,\n",
       "         -4.8001, -4.7275, -4.7330, -4.6153, -4.3808, -4.4026, -4.4726, -4.6803,\n",
       "         -4.5959, -4.6457, -4.6059, -4.6251, -4.8018, -4.5520, -4.3411, -4.4109,\n",
       "         -4.3294, -4.5700, -4.5917, -4.5648, -4.7794, -4.6084, -4.5451, -4.4370,\n",
       "         -4.4270, -4.5444, -4.6090, -4.6379, -4.3347, -4.5707, -4.6298, -4.5756,\n",
       "         -4.6244, -4.4828, -4.4875, -4.2320, -4.6694, -4.5753, -4.3752, -4.5922,\n",
       "         -4.5335, -4.5795, -4.6690, -4.7843, -4.8294, -4.6252, -4.7118, -4.5500,\n",
       "         -4.4018, -4.5484, -4.5771, -4.5708, -4.6722, -4.7600, -4.9052, -4.5318,\n",
       "         -4.5912, -4.3586, -4.6567, -4.0814, -4.4715]],\n",
       "       grad_fn=<SqueezeBackward1>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe7e50dc430>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bio_factoid_df.answer_text.map(lambda x: len(x)).value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_factoid_df['len'] = bio_factoid_df.answer_text.map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_contexts, squad_questions, squad_answers = read_squad('preproc_datasets/train-v2.0.json')\n",
    "squad_df = pd.DataFrame({'question': squad_questions,\n",
    "                        'context' :  squad_contexts,\n",
    "                        'answer_text' : [a['text'] for a in squad_answers]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_df['len'] = squad_df.answer_text.map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe7e5112580>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "squad_df.len.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question       0\n",
       "context        0\n",
       "answer_text    0\n",
       "len            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
