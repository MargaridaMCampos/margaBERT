{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model, tokenizer):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // len(train_dataloader) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) * args.num_train_epochs\n",
    "    \n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
    "        os.path.join(args.model_name_or_path, \"scheduler.pt\")\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\n",
    "        \"  Total train batch size = %d\",\n",
    "        args.train_batch_size\n",
    "    )\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 1\n",
    "    epochs_trained = 0\n",
    "\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\"\n",
    "    )\n",
    "\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "            model.train()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2],\n",
    "                \"labels\": (batch[7]==0.).to(torch.long), # is_impossible\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            # model outputs are always tuple in transformers (see doc)\n",
    "            loss = outputs[0]\n",
    "    \n",
    "            loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            # Log metrics\n",
    "            if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                logging_loss = tr_loss\n",
    "\n",
    "            # Save model checkpoint\n",
    "            if args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "                model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "                model_to_save.save_pretrained(output_dir)\n",
    "                tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "                torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "                torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "                \n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def \tmain():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Required parameters\n",
    "    parser.add_argument(\n",
    "        \"--train_file\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"SQuAD json for training. E.g., train-v1.1.json\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--predict_file\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help=\"SQuAD json for predictions. E.g., dev-v1.1.json or test-v1.1.json\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to pre-trained model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"The output directory where the model checkpoints will be written.\"\n",
    "    )\n",
    "    \n",
    "    # Other parameters\n",
    "    parser.add_argument(\n",
    "        \"--do_lower_case\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to lower case the input text. Should be True for uncased \"\n",
    "        \"models and False for cased models.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_seq_length\",\n",
    "        default=384,\n",
    "        type=int,\n",
    "        help=\"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "        \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "        \"than this will be padded.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--doc_stride\",\n",
    "        default=128,\n",
    "        type=int,\n",
    "        help=\"When splitting up a long document into chunks, how much stride to \"\n",
    "        \"take between chunks.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_query_length\",\n",
    "        default=64,\n",
    "        type=int,\n",
    "        help=\"The maximum number of tokens for the question. Questions longer than \"\n",
    "        \"this will be truncated to this length.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--do_train\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to run training.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--do_predict\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to run eval on the dev set.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\",\n",
    "        default=0,\n",
    "        type=int,\n",
    "        help=\"Random seed\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\",\n",
    "        default=32,\n",
    "        type=int,\n",
    "        help=\"Total batch size for training.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_batch_size\",\n",
    "        default=8,\n",
    "        type=int,\n",
    "        help=\"Total batch size for predictions.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--logging_steps\", \n",
    "        type=int, \n",
    "        default=500, \n",
    "        help=\"Log every X updates steps.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_steps\", \n",
    "        type=int, \n",
    "        default=500, \n",
    "        help=\"Save checkpoint every X updates steps.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        default=5e-5,\n",
    "        type=float,\n",
    "        help=\"The initial learning rate for Adam.\"\n",
    "    )\n",
    "    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
    "    parser.add_argument(\n",
    "        \"--num_train_epochs\",\n",
    "        default=3.0,\n",
    "        type=float,\n",
    "        help=\"Total number of training epochs to perform.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_steps\",\n",
    "        default=-1,\n",
    "        type=int,\n",
    "        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_all_checkpoints\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.doc_stride >= args.max_seq_length - args.max_query_length:\n",
    "        logger.warning(\n",
    "            \"WARNING - You've set a doc stride which may be superior to the document length in some \"\n",
    "            \"examples. This could result in errors when building features from the examples. Please reduce the doc \"\n",
    "            \"stride or increase the maximum length to ensure the features are correctly built.\"\n",
    "        )\n",
    "\n",
    "    # Setup CUDA\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process device: %s\", device\n",
    "    )\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(args)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    config = BertConfig.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "    )\n",
    "    tokenizer = BertTokenizer.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        do_lower_case=args.do_lower_case,\n",
    "    )\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        config=1,\n",
    "        from_tf = True\n",
    "    )\n",
    "    model.to(args.device)\n",
    "\n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    # Training\n",
    "    if args.do_train:\n",
    "        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n",
    "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Save the trained model and the tokenizer\n",
    "    if args.do_train:\n",
    "        # Create output directory if needed\n",
    "        if not os.path.exists(args.output_dir):\n",
    "            os.makedirs(args.output_dir)\n",
    "\n",
    "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "        model_to_save.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
